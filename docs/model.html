<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.1" />
<title>datasist.model API documentation</title>
<meta name="description" content="This module contains all functions relating to modeling in using sklearn library." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>datasist.model</code></h1>
</header>
<section id="section-intro">
<p>This module contains all functions relating to modeling in using sklearn library.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
This module contains all functions relating to modeling in using sklearn library.

&#39;&#39;&#39;
import platform

from sklearn.metrics import roc_curve, confusion_matrix, precision_score, accuracy_score, recall_score, f1_score, make_scorer
from sklearn.model_selection import KFold, cross_val_score
import numpy as np
import pandas as pd

if platform.system() == &#34;Darwin&#34;:
    import matplotlib as plt
    plt.use(&#39;TkAgg&#39;)
else:
    import matplotlib.pyplot as plt

import seaborn as sns

from .visualizations import plot_auc


def train_classifier(X_train = None, y_train=None, X_val=None, y_val=None, estimator=None, cross_validate=False, cv=5, show_roc_plot=True, save_plot=False):
    &#39;&#39;&#39;
    Train a classification estimator and calculate numerous performance metric.

    Parameters:
    ----------------------------
        X_train: Array, DataFrame, Series.

            The feature set (x) to use in training an estimator in other predict the outcome (y).

        y_train: Series, 1-d array, list

            The ground truth value for the train dataset

        X_val: Array, DataFrame. Series.

            The feature set (x) to use in validating a trained estimator.

        y_val: Series, 1-d array, list

            The ground truth value for the validation dataset.

        estimator: sklearn estimator.

            Sklearn estimator that implements the fit and predict functions.

        cross_validate: Bool, default False

            Use a cross validation strategy or not.
        
        cv: Int, default 5

            Number of folds to use in cross validation.
        
        show_roc_curve: Bool, default True.

            Plot a ROC plot showing estimator performance.
        
        save_plot: Bool, default False.
            Save the plot as a png file.
    
&#39;&#39;&#39;
    if X_train is None:
        raise ValueError(&#34;X_train: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if y_train is None:
        raise ValueError(&#34;y_train: Expecting a Series/ numpy1D array, got &#39;None&#39;&#34;)

    #initialize variables to hold calculations
    pred, acc, f1, precision, recall, confusion_mat = 0, 0, 0, 0, 0, None

    if cross_validate:
        dict_scorers  = {&#39;acc&#39; : accuracy_score,
                        &#39;f1&#39; : f1_score,
                        &#39;precision&#39;: precision_score, 
                        &#39;recall&#39; : recall_score
                        }


        metric_names = [&#39;Accuracy&#39;, &#39;F1_score&#39;, &#39;Precision&#39;, &#39;Recall&#39;]

        for metric_name, scorer in zip(metric_names, dict_scorers):
            cv_score = np.mean(cross_val_score(estimator, X_train, y_train, scoring=make_scorer(dict_scorers[scorer]),cv=cv))
            print(&#34;{} is {}&#34;.format(metric_name,  round(cv_score * 100, 4)))

        #TODO Add cross validation function for confusion matrix
  
    else:
        if X_val is None:
            raise ValueError(&#34;X_val: Expecting a DataFrame/ numpy array, got &#39;None&#39;&#34;)
        
        if y_val is None:
            raise ValueError(&#34;y_val: Expecting a Series/ numpy1D array, got &#39;None&#39;&#34;)

        estimator.fit(X_train, y_train)
        pred = estimator.predict(X_val)
        get_classification_report(y_val, pred, show_roc_plot, save_plot)



def plot_feature_importance(estimator=None, col_names=None):
    &#39;&#39;&#39;
    Plots the feature importance from a trained scikit learn estimator
    as a bar chart.

    Parameters:
    -----------
        estimator: scikit  learn estimator.

            Model that has been fit and contains the feature_importance_ attribute.

        col_names: list

            The names of the columns. Must map unto feature importance array.

    Returns:
    --------
        Matplotlib figure showing feature importances
    &#39;&#39;&#39;
    if estimator is None:
        raise ValueError(&#34;estimator: Expecting an estimator that implements the fit api, got None&#34;)
    if col_names is None:
        raise ValueError(&#34;col_names: Expecting a list of column names, got &#39;None&#39;&#34;)
    
    if len(col_names) != len(estimator.feature_importances_):
        raise ValueError(&#34;col_names: Lenght of col_names must match lenght of feature importances&#34;)

    imps = estimator.feature_importances_
    feats_imp = pd.DataFrame({&#34;features&#34;: col_names, &#34;importance&#34;: imps}).sort_values(by=&#39;importance&#39;, ascending=False)
    sns.barplot(x=&#39;features&#39;, y=&#39;importance&#39;, data=feats_imp)
    plt.xticks(rotation=90)
    plt.title(&#34;Feature importance plot&#34;)
    plt.show()


def make_submission_csv(estimator=None, X_train=None, y_train=None, test_data=None,
                  sample_submision_file=None, sub_col_name=None, name=&#34;final_submission&#34;):
    &#39;&#39;&#39;
    Train a estimator and makes prediction with it on the final test set. Also
    returns a sample submission file for data science competitions
    
    Parameters:
    -----------------------
    estimator: Sklearn estimator.

        An sklearn estimator that implements the fit and predict functions.

    X_train: Array, DataFrame, Series.

        The feature set (x) to use in training an estimator to predict the outcome (y).

    y_train: Series, 1-d array, list

        The ground truth value for the train dataset

    test_data: Array, DataFrame. Series.

        The final test set (x) to predict on.
    
    sample_submision_file: DataFrame, Series.

        A sample csv file provided by data science competition hosts to use in creating your final submission.

    sub_col_name: String.

        Name of the prediction column in the sample submission file.

    name: String.

        Name of the created submission file.

    Return:

        Csv file saved in current working directory
    &#39;&#39;&#39;
    estimator.fit(X_train, y_train)
    pred = estimator.predict(test_data)

    sub = sample_submision_file
    sub[sub_col_name] = pred
    sub.to_csv(name + &#39;.csv&#39;, index=False)
    print(&#34;File has been saved to current working directory&#34;)



def get_classification_report(y_train=None, prediction=None, show_roc_plot=True, save_plot=False):
    &#39;&#39;&#39;
    Generates performance report for a classification problem.

    Parameters:
    ------------------
    y_train: Array, series, list.

        The truth/ground value from the train data set.
    
    prediction: Array, series, list.

        The predicted value by a trained model.

    show_roc_plot: Bool, default True.

        Show the model ROC curve.

    save_plot: Bool, default True.

        Save the plot to the current working directory.

    &#39;&#39;&#39;
    acc = accuracy_score(y_train, prediction)
    f1 = f1_score(y_train, prediction)
    precision = precision_score(y_train, prediction)
    recall = recall_score(y_train, prediction)
    confusion_mat = confusion_matrix(y_train, prediction)

    print(&#34;Accuracy is &#34;, round(acc * 100))
    print(&#34;F1 score is &#34;, round(f1 * 100))
    print(&#34;Precision is &#34;, round(precision * 100))
    print(&#34;Recall is &#34;, round(recall * 100))
    print(&#34;*&#34; * 100)
    print(&#34;confusion Matrix&#34;)
    print(&#39;                 Score positive    Score negative&#39;)
    print(&#39;Actual positive    %6d&#39; % confusion_mat[0,0] + &#39;             %5d&#39; % confusion_mat[0,1])
    print(&#39;Actual negative    %6d&#39; % confusion_mat[1,0] + &#39;             %5d&#39; % confusion_mat[1,1])
    print(&#39;&#39;)

    if show_roc_plot:        
        plot_auc(y_train, prediction)

        if save_plot:
            plt.savefig(&#34;roc_plot.png&#34;)


def compare_model(models_list=None, x_train=None, y_train=None, scoring_metric=None, scoring_cv=3, silenced=True, plot=True):
    &#34;&#34;&#34;
    Train multiple user-defined model and display report based on defined metric. Enables user to pick the best base model for a problem.

    Parameters
    ----------------
        models_list: list

            a list of models to be trained

        x_train: Array, DataFrame, Series

            The feature set (x) to use in training an estimator to predict the outcome (y).

        y_train: Series, 1-d array, list

            The ground truth value for the train dataset

        scoring_metric: str

            Metric to use in scoring the model

        scoring_cv: int

            default value is 3

    Returns
    ---------------
    a tuple of fitted_model and the model evaluation scores
    &#34;&#34;&#34;
    # if not _same_model:
    #     raise ValueError(&#34;model_list: models must be of the same class. Cannot use a classifier and a regressor.&#34;)


    if models_list is None or len(models_list) &lt; 1:
        raise ValueError(&#34;model_list: model_list can&#39;t be &#39;None&#39; or empty&#34;)

    if x_train is None:
        raise ValueError(&#34;x_train: features can&#39;t be &#39;None&#39; or empty&#34;)

    if y_train is None:
        raise ValueError(&#34;y_train: features can&#39;t be &#39;None&#39; or empty&#34;)



    fitted_model = []
    model_scores = []
    model_names = []

    for i, model in enumerate(models_list):
        if silenced is not True:
            print(f&#34;Fitting {type(model).__name__} ... \n&#34;)
        model.fit(x_train, y_train)
        # append fitted model into list
        fitted_model.append(model)
        model_score = np.mean(cross_val_score(model, x_train, y_train, scoring=scoring_metric, cv=scoring_cv))
        model_scores.append(model_score)
        model_names.append(type(fitted_model[i]).__name__)
            
    if plot:
        sns.pointplot(y=model_scores, x=model_names)
        plt.xticks(rotation=90)
        plt.title(&#34;Model comparison plot&#34;)
        plt.show()

    return fitted_model, model_scores</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="datasist.model.compare_model"><code class="name flex">
<span>def <span class="ident">compare_model</span></span>(<span>models_list=None, x_train=None, y_train=None, scoring_metric=None, scoring_cv=3, silenced=True, plot=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Train multiple user-defined model and display report based on defined metric. Enables user to pick the best base model for a problem.</p>
<h2 id="parameters">Parameters</h2>
<pre><code>models_list: list

    a list of models to be trained

x_train: Array, DataFrame, Series

    The feature set (x) to use in training an estimator to predict the outcome (y).

y_train: Series, 1-d array, list

    The ground truth value for the train dataset

scoring_metric: str

    Metric to use in scoring the model

scoring_cv: int

    default value is 3
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>a</code> <code>tuple</code> of <code>fitted_model</code> <code>and</code> <code>the</code> <code>model</code> <code>evaluation</code> <code>scores</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compare_model(models_list=None, x_train=None, y_train=None, scoring_metric=None, scoring_cv=3, silenced=True, plot=True):
    &#34;&#34;&#34;
    Train multiple user-defined model and display report based on defined metric. Enables user to pick the best base model for a problem.

    Parameters
    ----------------
        models_list: list

            a list of models to be trained

        x_train: Array, DataFrame, Series

            The feature set (x) to use in training an estimator to predict the outcome (y).

        y_train: Series, 1-d array, list

            The ground truth value for the train dataset

        scoring_metric: str

            Metric to use in scoring the model

        scoring_cv: int

            default value is 3

    Returns
    ---------------
    a tuple of fitted_model and the model evaluation scores
    &#34;&#34;&#34;
    # if not _same_model:
    #     raise ValueError(&#34;model_list: models must be of the same class. Cannot use a classifier and a regressor.&#34;)


    if models_list is None or len(models_list) &lt; 1:
        raise ValueError(&#34;model_list: model_list can&#39;t be &#39;None&#39; or empty&#34;)

    if x_train is None:
        raise ValueError(&#34;x_train: features can&#39;t be &#39;None&#39; or empty&#34;)

    if y_train is None:
        raise ValueError(&#34;y_train: features can&#39;t be &#39;None&#39; or empty&#34;)



    fitted_model = []
    model_scores = []
    model_names = []

    for i, model in enumerate(models_list):
        if silenced is not True:
            print(f&#34;Fitting {type(model).__name__} ... \n&#34;)
        model.fit(x_train, y_train)
        # append fitted model into list
        fitted_model.append(model)
        model_score = np.mean(cross_val_score(model, x_train, y_train, scoring=scoring_metric, cv=scoring_cv))
        model_scores.append(model_score)
        model_names.append(type(fitted_model[i]).__name__)
            
    if plot:
        sns.pointplot(y=model_scores, x=model_names)
        plt.xticks(rotation=90)
        plt.title(&#34;Model comparison plot&#34;)
        plt.show()

    return fitted_model, model_scores</code></pre>
</details>
</dd>
<dt id="datasist.model.get_classification_report"><code class="name flex">
<span>def <span class="ident">get_classification_report</span></span>(<span>y_train=None, prediction=None, show_roc_plot=True, save_plot=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Generates performance report for a classification problem.</p>
<h2 id="parameters">Parameters:</h2>
<p>y_train: Array, series, list.</p>
<pre><code>The truth/ground value from the train data set.
</code></pre>
<p>prediction: Array, series, list.</p>
<pre><code>The predicted value by a trained model.
</code></pre>
<p>show_roc_plot: Bool, default True.</p>
<pre><code>Show the model ROC curve.
</code></pre>
<p>save_plot: Bool, default True.</p>
<pre><code>Save the plot to the current working directory.
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_classification_report(y_train=None, prediction=None, show_roc_plot=True, save_plot=False):
    &#39;&#39;&#39;
    Generates performance report for a classification problem.

    Parameters:
    ------------------
    y_train: Array, series, list.

        The truth/ground value from the train data set.
    
    prediction: Array, series, list.

        The predicted value by a trained model.

    show_roc_plot: Bool, default True.

        Show the model ROC curve.

    save_plot: Bool, default True.

        Save the plot to the current working directory.

    &#39;&#39;&#39;
    acc = accuracy_score(y_train, prediction)
    f1 = f1_score(y_train, prediction)
    precision = precision_score(y_train, prediction)
    recall = recall_score(y_train, prediction)
    confusion_mat = confusion_matrix(y_train, prediction)

    print(&#34;Accuracy is &#34;, round(acc * 100))
    print(&#34;F1 score is &#34;, round(f1 * 100))
    print(&#34;Precision is &#34;, round(precision * 100))
    print(&#34;Recall is &#34;, round(recall * 100))
    print(&#34;*&#34; * 100)
    print(&#34;confusion Matrix&#34;)
    print(&#39;                 Score positive    Score negative&#39;)
    print(&#39;Actual positive    %6d&#39; % confusion_mat[0,0] + &#39;             %5d&#39; % confusion_mat[0,1])
    print(&#39;Actual negative    %6d&#39; % confusion_mat[1,0] + &#39;             %5d&#39; % confusion_mat[1,1])
    print(&#39;&#39;)

    if show_roc_plot:        
        plot_auc(y_train, prediction)

        if save_plot:
            plt.savefig(&#34;roc_plot.png&#34;)</code></pre>
</details>
</dd>
<dt id="datasist.model.make_submission_csv"><code class="name flex">
<span>def <span class="ident">make_submission_csv</span></span>(<span>estimator=None, X_train=None, y_train=None, test_data=None, sample_submision_file=None, sub_col_name=None, name='final_submission')</span>
</code></dt>
<dd>
<section class="desc"><p>Train a estimator and makes prediction with it on the final test set. Also
returns a sample submission file for data science competitions</p>
<h2 id="parameters">Parameters:</h2>
<p>estimator: Sklearn estimator.</p>
<pre><code>An sklearn estimator that implements the fit and predict functions.
</code></pre>
<p>X_train: Array, DataFrame, Series.</p>
<pre><code>The feature set (x) to use in training an estimator to predict the outcome (y).
</code></pre>
<p>y_train: Series, 1-d array, list</p>
<pre><code>The ground truth value for the train dataset
</code></pre>
<p>test_data: Array, DataFrame. Series.</p>
<pre><code>The final test set (x) to predict on.
</code></pre>
<p>sample_submision_file: DataFrame, Series.</p>
<pre><code>A sample csv file provided by data science competition hosts to use in creating your final submission.
</code></pre>
<p>sub_col_name: String.</p>
<pre><code>Name of the prediction column in the sample submission file.
</code></pre>
<p>name: String.</p>
<pre><code>Name of the created submission file.
</code></pre>
<h2 id="return">Return</h2>
<p>Csv file saved in current working directory</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_submission_csv(estimator=None, X_train=None, y_train=None, test_data=None,
                  sample_submision_file=None, sub_col_name=None, name=&#34;final_submission&#34;):
    &#39;&#39;&#39;
    Train a estimator and makes prediction with it on the final test set. Also
    returns a sample submission file for data science competitions
    
    Parameters:
    -----------------------
    estimator: Sklearn estimator.

        An sklearn estimator that implements the fit and predict functions.

    X_train: Array, DataFrame, Series.

        The feature set (x) to use in training an estimator to predict the outcome (y).

    y_train: Series, 1-d array, list

        The ground truth value for the train dataset

    test_data: Array, DataFrame. Series.

        The final test set (x) to predict on.
    
    sample_submision_file: DataFrame, Series.

        A sample csv file provided by data science competition hosts to use in creating your final submission.

    sub_col_name: String.

        Name of the prediction column in the sample submission file.

    name: String.

        Name of the created submission file.

    Return:

        Csv file saved in current working directory
    &#39;&#39;&#39;
    estimator.fit(X_train, y_train)
    pred = estimator.predict(test_data)

    sub = sample_submision_file
    sub[sub_col_name] = pred
    sub.to_csv(name + &#39;.csv&#39;, index=False)
    print(&#34;File has been saved to current working directory&#34;)</code></pre>
</details>
</dd>
<dt id="datasist.model.plot_feature_importance"><code class="name flex">
<span>def <span class="ident">plot_feature_importance</span></span>(<span>estimator=None, col_names=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Plots the feature importance from a trained scikit learn estimator
as a bar chart.</p>
<h2 id="parameters">Parameters:</h2>
<pre><code>estimator: scikit  learn estimator.

    Model that has been fit and contains the feature_importance_ attribute.

col_names: list

    The names of the columns. Must map unto feature importance array.
</code></pre>
<h2 id="returns">Returns:</h2>
<pre><code>Matplotlib figure showing feature importances
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_feature_importance(estimator=None, col_names=None):
    &#39;&#39;&#39;
    Plots the feature importance from a trained scikit learn estimator
    as a bar chart.

    Parameters:
    -----------
        estimator: scikit  learn estimator.

            Model that has been fit and contains the feature_importance_ attribute.

        col_names: list

            The names of the columns. Must map unto feature importance array.

    Returns:
    --------
        Matplotlib figure showing feature importances
    &#39;&#39;&#39;
    if estimator is None:
        raise ValueError(&#34;estimator: Expecting an estimator that implements the fit api, got None&#34;)
    if col_names is None:
        raise ValueError(&#34;col_names: Expecting a list of column names, got &#39;None&#39;&#34;)
    
    if len(col_names) != len(estimator.feature_importances_):
        raise ValueError(&#34;col_names: Lenght of col_names must match lenght of feature importances&#34;)

    imps = estimator.feature_importances_
    feats_imp = pd.DataFrame({&#34;features&#34;: col_names, &#34;importance&#34;: imps}).sort_values(by=&#39;importance&#39;, ascending=False)
    sns.barplot(x=&#39;features&#39;, y=&#39;importance&#39;, data=feats_imp)
    plt.xticks(rotation=90)
    plt.title(&#34;Feature importance plot&#34;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="datasist.model.train_classifier"><code class="name flex">
<span>def <span class="ident">train_classifier</span></span>(<span>X_train=None, y_train=None, X_val=None, y_val=None, estimator=None, cross_validate=False, cv=5, show_roc_plot=True, save_plot=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Train a classification estimator and calculate numerous performance metric.</p>
<h2 id="parameters">Parameters:</h2>
<pre><code>X_train: Array, DataFrame, Series.

    The feature set (x) to use in training an estimator in other predict the outcome (y).

y_train: Series, 1-d array, list

    The ground truth value for the train dataset

X_val: Array, DataFrame. Series.

    The feature set (x) to use in validating a trained estimator.

y_val: Series, 1-d array, list

    The ground truth value for the validation dataset.

estimator: sklearn estimator.

    Sklearn estimator that implements the fit and predict functions.

cross_validate: Bool, default False

    Use a cross validation strategy or not.

cv: Int, default 5

    Number of folds to use in cross validation.

show_roc_curve: Bool, default True.

    Plot a ROC plot showing estimator performance.

save_plot: Bool, default False.
    Save the plot as a png file.
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_classifier(X_train = None, y_train=None, X_val=None, y_val=None, estimator=None, cross_validate=False, cv=5, show_roc_plot=True, save_plot=False):
    &#39;&#39;&#39;
    Train a classification estimator and calculate numerous performance metric.

    Parameters:
    ----------------------------
        X_train: Array, DataFrame, Series.

            The feature set (x) to use in training an estimator in other predict the outcome (y).

        y_train: Series, 1-d array, list

            The ground truth value for the train dataset

        X_val: Array, DataFrame. Series.

            The feature set (x) to use in validating a trained estimator.

        y_val: Series, 1-d array, list

            The ground truth value for the validation dataset.

        estimator: sklearn estimator.

            Sklearn estimator that implements the fit and predict functions.

        cross_validate: Bool, default False

            Use a cross validation strategy or not.
        
        cv: Int, default 5

            Number of folds to use in cross validation.
        
        show_roc_curve: Bool, default True.

            Plot a ROC plot showing estimator performance.
        
        save_plot: Bool, default False.
            Save the plot as a png file.
    
&#39;&#39;&#39;
    if X_train is None:
        raise ValueError(&#34;X_train: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if y_train is None:
        raise ValueError(&#34;y_train: Expecting a Series/ numpy1D array, got &#39;None&#39;&#34;)

    #initialize variables to hold calculations
    pred, acc, f1, precision, recall, confusion_mat = 0, 0, 0, 0, 0, None

    if cross_validate:
        dict_scorers  = {&#39;acc&#39; : accuracy_score,
                        &#39;f1&#39; : f1_score,
                        &#39;precision&#39;: precision_score, 
                        &#39;recall&#39; : recall_score
                        }


        metric_names = [&#39;Accuracy&#39;, &#39;F1_score&#39;, &#39;Precision&#39;, &#39;Recall&#39;]

        for metric_name, scorer in zip(metric_names, dict_scorers):
            cv_score = np.mean(cross_val_score(estimator, X_train, y_train, scoring=make_scorer(dict_scorers[scorer]),cv=cv))
            print(&#34;{} is {}&#34;.format(metric_name,  round(cv_score * 100, 4)))

        #TODO Add cross validation function for confusion matrix
  
    else:
        if X_val is None:
            raise ValueError(&#34;X_val: Expecting a DataFrame/ numpy array, got &#39;None&#39;&#34;)
        
        if y_val is None:
            raise ValueError(&#34;y_val: Expecting a Series/ numpy1D array, got &#39;None&#39;&#34;)

        estimator.fit(X_train, y_train)
        pred = estimator.predict(X_val)
        get_classification_report(y_val, pred, show_roc_plot, save_plot)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
    <h1><img src="datasist.png" alt="logo"></h1>
    <div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="datasist" href="index.html">datasist</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="datasist.model.compare_model" href="#datasist.model.compare_model">compare_model</a></code></li>
<li><code><a title="datasist.model.get_classification_report" href="#datasist.model.get_classification_report">get_classification_report</a></code></li>
<li><code><a title="datasist.model.make_submission_csv" href="#datasist.model.make_submission_csv">make_submission_csv</a></code></li>
<li><code><a title="datasist.model.plot_feature_importance" href="#datasist.model.plot_feature_importance">plot_feature_importance</a></code></li>
<li><code><a title="datasist.model.train_classifier" href="#datasist.model.train_classifier">train_classifier</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>