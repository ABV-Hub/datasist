<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.1" />
<title>datasist.feature_engineering API documentation</title>
<meta name="description" content="This module contains all functions relating to feature engineering" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>datasist.feature_engineering</code></h1>
</header>
<section id="section-intro">
<p>This module contains all functions relating to feature engineering</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
This module contains all functions relating to feature engineering
&#39;&#39;&#39;

import pandas as pd
import numpy as np
from .structdata import get_cat_feats, get_num_feats, get_date_cols


def drop_missing(data=None, percent=99):
    &#39;&#39;&#39;
    Drops missing columns with [percent] of missing data.

    Parameters:
    data: Pandas DataFrame or Series.
    percent: float, Default 99
        Percentage of missing values to be in a column before it is eligible for removal.

    Returns:
        Pandas DataFrame or Series.
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    missing_percent = (data.isna().sum() / data.shape[0]) * 100
    cols_2_drop = missing_percent[missing_percent.values &gt; percent].index
    print(&#34;Dropped {}&#34;.format(list(cols_2_drop)))
    #Drop missing values
    data.drop(cols_2_drop, axis=1, inplace=True)



def drop_redundant(data):
    &#39;&#39;&#39;
    Removes features with the same value in all cell. 
    Drops feature If Nan is the second unique class as well.
    Parameters:
        data: DataFrame or named series
    
    Returns:
        DataFrame or named series
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    #get columns
    cols_2_drop = _nan_in_class(data)
    print(&#34;Dropped {}&#34;.format(cols_2_drop))
    data.drop(cols_2_drop, axis=1, inplace=True)
    
    

def _nan_in_class(data):
    cols = []
    for col in data.columns:
        if len(data[col].unique()) == 1:
            cols.append(col)

        if len(data[col].unique()) == 2:
            if np.nan in list(data[col].unique()):
                cols.append(col)

    return cols



def fill_missing_cats(data=None, cat_features=None, missing_encoding=None):
    &#39;&#39;&#39;
    Fill missing values using the mode of the categorical features.
    Parameters:
    ----------
    data: DataFrame or name Series.
        Data set to perform operation on.
    cat_features: List, Series, Array.
        categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.
    missing_encoding: List, Series, Array.
            Values used in place of missing. Popular formats are [-1, -999, -99, &#39;&#39;, &#39; &#39;]
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)

    if cat_features is None:
        cat_features = get_cat_feats(data)

    temp_data = data.copy()
    #change all possible missing values to NaN
    if missing_encoding is None:
        missing_encoding = [&#39;&#39;, &#39; &#39;, -99, -999]

    temp_data.replace(missing_encoding, np.NaN, inplace=True)
    
    for col in cat_features:
        most_freq = temp_data[col].mode()[0]
        temp_data[col] = temp_data[col].replace(np.NaN, most_freq)
    
    return temp_data


def fill_missing_num(data=None, features=None, method=&#39;mean&#39;):
    &#39;&#39;&#39;
    fill missing values in numerical columns with specified [method] value
    Parameters:
    ----------
    data: DataFrame or name Series.
        The data set to fill
    features: list.
        List of columns to fill
    method: str, Default &#39;mean&#39;
        method to use in calculating fill value.
    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if features is None:
        #get numerical features with missing values
        num_feats = get_num_feats(data)
        temp_data = data[num_feats].isna().sum()
        features = list(temp_data[num_feats][temp_data[num_feats] &gt; 0].index)
        print(&#34;Found {} with missing values.&#34;.format(features))

    for feat in features:
        if method is &#39;mean&#39;:
            mean = data[feat].mean()
            data[feat].fillna(mean, inplace=True)
        elif method is &#39;median&#39;:
            median = data[feat].median()
            data[feat].fillna(median, inplace=True)
        elif method is &#39;mode&#39;:
            mode = data[feat].mode()[0]
            data[feat].fillna(mode, inplace=True)
   
    return &#34;Filled all missing values successfully&#34;


def merge_groupby(data=None, cat_features=None, statistics=None, col_to_merge=None):
    &#39;&#39;&#39;
    Performs a groupby on the specified categorical features and merges
    the result to the original dataframe.

    Parameter:
    ----------
    data: DataFrame
        Data set to perform operation on.
    cat_features: list, series, 1D-array
        categorical features to groupby.
    statistics: list, series, 1D-array, Default [&#39;mean&#39;, &#39;count]
        aggregates to perform on grouped data.
    col_to_merge: str
        The column to merge on the dataset. Must be present in the data set.
    Returns:
        Merged dataframe.

    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if statistics is None:     
        statistics = [&#39;mean&#39;, &#39;count&#39;]
    
    if cat_features is None:
        cat_features = get_num_feats(data)

    if col_to_merge is None:
        raise ValueError(&#34;col_to_merge: Expecting a string [column to merge on], got &#39;None&#39;&#34;)

    
    df = data.copy()
    
    for cat in cat_features:      
        temp = df.groupby([cat]).agg(statistics)[col_to_merge]
        #rename columns
        temp = temp.rename(columns={&#39;mean&#39;: cat + &#39;_&#39; + col_to_merge + &#39;_mean&#39;, &#39;count&#39;: cat + &#39;_&#39; + col_to_merge +  &#34;_count&#34;})
        #merge the data sets
        df = df.merge(temp, how=&#39;left&#39;, on=cat)
    
    
    return df


def get_qcut(data=None, col=None, q=None, duplicates=&#39;drop&#39;, return_type=&#39;float64&#39;):
    &#39;&#39;&#39;
    Cuts a series into bins using the pandas qcut function
    and returns the resulting bins as a series for merging.
    Parameter:
    ----------
    data: DataFrame, named Series
        Data set to perform operation on.
    col: str
        column to cut/binnarize.
    q: integer or array of quantiles
        Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately
        array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.
    duplicates: Default &#39;drop&#39;,
        If bin edges are not unique drop non-uniques.
    return_type: dtype, Default (float64)
        Dtype of series to return. One of [float64, str, int64]
    
    Returns:
    --------
    Series, 1D-Array
        binned series
    &#39;&#39;&#39;

    temp_df = pd.qcut(data[col], q=q, duplicates=duplicates).to_frame().astype(&#39;str&#39;)
    #retrieve only the qcut categories
    df = temp_df[col].str.split(&#39;,&#39;).apply(lambda x: x[0][1:]).astype(return_type)
    
    return df


def create_balanced_data(data=None, target=None, categories=None, class_sizes=None, replacement=False ):
    &#39;&#39;&#39;
    Creates a balanced data set from an imbalanced one. Used in a classification task.

    Parameter:
    data: DataFrame, name series.
        The imbalanced dataset.
    target: str
        Name of the target column.
    categories: list
        Unique categories in the target column. If not set, we use infer the unique categories in the column.
    class_sizes: list
        Size of each specified class. Must be in order with categoriess parameter.
    replacement: bool, Default True.
        samples with or without replacement.
    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if target is None:
        raise ValueError(&#34;target: Expecting a String got &#39;None&#39;&#34;)

    if categories is None:
        categories = list(data[target].unique())
    
    if class_sizes is None:
        #set size for each class to same value
        temp_val = int(data.shape[0] / len(data[target].unique()))
        class_sizes = [temp_val for _ in list(data[target].unique())]

    
    temp_data = data.copy()
    data_category = []
    data_class_indx = []
    
    #get data corrresponding to each of the categories
    for cat in categories: 
        data_category.append(temp_data[temp_data[target] == cat])
    
    #sample and get the index corresponding to each category
    for class_size, cat in zip(class_sizes, data_category):
        data_class_indx.append(cat.sample(class_size, replace=True).index)
        
    #concat data together
    new_data = pd.concat([temp_data.loc[indx] for indx in data_class_indx], ignore_index=True).sample(sum(class_sizes)).reset_index(drop=True)
    
    if not replacement:
        for indx in data_class_indx:
            temp_data.drop(indx, inplace=True)
            
        
    return new_data



def to_date(data):
    &#39;&#39;&#39;
    Automatically convert all date time columns to pandas Datetime format
    &#39;&#39;&#39;

    date_cols = get_date_cols(data)
    for col in date_cols:
        data[col] = pd.to_datetime(data[col])
    
    return data


def haversine_distance(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Haversine distance between two location with latitude and longitude.
    The haversine distance is the great-circle distance between two points on a sphere given their longitudes and latitudes.
    
    Parameter:
    lat1: scalar,float
        Start point latitude of the location.
    lat2: scalar,float 
        End point latitude of the location.
    long1: scalar,float
        Start point longitude of the location.
    long2: scalar,float 
        End point longitude of the location.

    Returns: Series
        The Harversine distance between (lat1, lat2), (long1, long2)
    
    &#39;&#39;&#39;

    lat1, long1, lat2, long2 = map(np.radians, (lat1, long1, lat2, long2))
    AVG_EARTH_RADIUS = 6371  # in km
    lat = lat2 - lat1
    lng = long2 - long1
    distance = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2
    harvesine_distance = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(distance))
    harvesine_distance_df = pd.Series(harvesine_distance)
    return harvesine_distance_df


def manhattan_distance(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Manhattan distance between two points.
    It is the sum of horizontal and vertical distance between any two points given their latitudes and longitudes. 
    Parameter:
    lat1: scalar,float
        Start point latitude of the location.
    lat2: scalar,float 
        End point latitude of the location.
    long1: scalar,float
        Start point longitude of the location.
    long2: scalar,float 
        End point longitude of the location.

    Returns: Series
        The Manhattan distance between (lat1, lat2) and (long1, long2)
    
    &#39;&#39;&#39;
    a = np.abs(lat2 -lat1)
    b = np.abs(long1 - long2)
    manhattan_distance = a + b
    manhattan_distance_df = pd.Series(manhattan_distance)
    return manhattan_distance_df
    

def bearing(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Bearing  between two points.
    The bearing is the compass direction to travel from a starting point, and must be within the range 0 to 360.    Parameter:
    lat1: scalar,float
        Start point latitude of the location.
    lat2: scalar,float 
        End point latitude of the location.
    long1: scalar,float
        Start point longitude of the location.
    long2: scalar,float 
        End point longitude of the location.

    Returns: Series
        The Bearing between (lat1, lat2) and (long1, long2)
    
    &#39;&#39;&#39;
    AVG_EARTH_RADIUS = 6371
    long_delta = np.radians(long2 - long1)
    lat1, long1, lat2, long2 = map(np.radians, (lat1, long1, lat2, long2))
    y = np.sin(long_delta) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(long_delta)
    bearing = np.degrees(np.arctan2(y, x))
    bearing_df = pd.Series(bearing)
    return bearing_df
    

def get_location_center(point1, point2):
    &#39;&#39;&#39;
    Calculates the center between two points.

    point1: list, series, scalar
        End point latitude of the location.
    long1: list, series, scalar
        Start point longitude of the location.
    long2: list, series, scalar
        End point longitude of the location.

    Returns: Series
        The center between point1 and point2
    
    &#39;&#39;&#39;
    center = (point1 + point2) / 2
    center_df = pd.Series(center)
    return center_df

    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="datasist.feature_engineering.bearing"><code class="name flex">
<span>def <span class="ident">bearing</span></span>(<span>lat1, long1, lat2, long2)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the Bearing
between two points.
The bearing is the compass direction to travel from a starting point, and must be within the range 0 to 360.
Parameter:
lat1: scalar,float
Start point latitude of the location.
lat2: scalar,float
End point latitude of the location.
long1: scalar,float
Start point longitude of the location.
long2: scalar,float
End point longitude of the location.</p>
<p>Returns: Series
The Bearing between (lat1, lat2) and (long1, long2)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bearing(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Bearing  between two points.
    The bearing is the compass direction to travel from a starting point, and must be within the range 0 to 360.<br>
    Parameter: <br>
    lat1: scalar,float<br>
        Start point latitude of the location.<br>
    lat2: scalar,float <br>
        End point latitude of the location.<br>
    long1: scalar,float<br>
        Start point longitude of the location.<br>
    long2: scalar,float <br>
        End point longitude of the location.<br><br>

    Returns: Series<br>
        The Bearing between (lat1, lat2) and (long1, long2)<br>
    
    &#39;&#39;&#39;
    AVG_EARTH_RADIUS = 6371
    long_delta = np.radians(long2 - long1)
    lat1, long1, lat2, long2 = map(np.radians, (lat1, long1, lat2, long2))
    y = np.sin(long_delta) * np.cos(lat2)
    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(long_delta)
    bearing = np.degrees(np.arctan2(y, x))
    bearing_df = pd.Series(bearing)
    return bearing_df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.create_balanced_data"><code class="name flex">
<span>def <span class="ident">create_balanced_data</span></span>(<span>data=None, target=None, categories=None, class_sizes=None, replacement=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a balanced data set from an imbalanced one. Used in a classification task.</p>
<p>Parameter:
data: DataFrame, name series.
The imbalanced dataset.
target: str
Name of the target column.
categories: list
Unique categories in the target column. If not set, we use infer the unique categories in the column.
class_sizes: list
Size of each specified class. Must be in order with categoriess parameter.
replacement: bool, Default True.
samples with or without replacement.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_balanced_data(data=None, target=None, categories=None, class_sizes=None, replacement=False ):
    &#39;&#39;&#39;
    Creates a balanced data set from an imbalanced one. Used in a classification task.

    Parameter:
    data: DataFrame, name series.
        The imbalanced dataset.
    target: str
        Name of the target column.
    categories: list
        Unique categories in the target column. If not set, we use infer the unique categories in the column.
    class_sizes: list
        Size of each specified class. Must be in order with categoriess parameter.
    replacement: bool, Default True.
        samples with or without replacement.
    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if target is None:
        raise ValueError(&#34;target: Expecting a String got &#39;None&#39;&#34;)

    if categories is None:
        categories = list(data[target].unique())
    
    if class_sizes is None:
        #set size for each class to same value
        temp_val = int(data.shape[0] / len(data[target].unique()))
        class_sizes = [temp_val for _ in list(data[target].unique())]

    
    temp_data = data.copy()
    data_category = []
    data_class_indx = []
    
    #get data corrresponding to each of the categories
    for cat in categories: 
        data_category.append(temp_data[temp_data[target] == cat])
    
    #sample and get the index corresponding to each category
    for class_size, cat in zip(class_sizes, data_category):
        data_class_indx.append(cat.sample(class_size, replace=True).index)
        
    #concat data together
    new_data = pd.concat([temp_data.loc[indx] for indx in data_class_indx], ignore_index=True).sample(sum(class_sizes)).reset_index(drop=True)
    
    if not replacement:
        for indx in data_class_indx:
            temp_data.drop(indx, inplace=True)
            
        
    return new_data</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.drop_missing"><code class="name flex">
<span>def <span class="ident">drop_missing</span></span>(<span>data=None, percent=99)</span>
</code></dt>
<dd>
<section class="desc"><p>Drops missing columns with [percent] of missing data.</p>
<p>Parameters:
data: Pandas DataFrame or Series.
percent: float, Default 99
Percentage of missing values to be in a column before it is eligible for removal.</p>
<h2 id="returns">Returns</h2>
<p>Pandas DataFrame or Series.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_missing(data=None, percent=99):
    &#39;&#39;&#39;
    Drops missing columns with [percent] of missing data.

    Parameters:
    data: Pandas DataFrame or Series.
    percent: float, Default 99
        Percentage of missing values to be in a column before it is eligible for removal.

    Returns:
        Pandas DataFrame or Series.
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    missing_percent = (data.isna().sum() / data.shape[0]) * 100
    cols_2_drop = missing_percent[missing_percent.values &gt; percent].index
    print(&#34;Dropped {}&#34;.format(list(cols_2_drop)))
    #Drop missing values
    data.drop(cols_2_drop, axis=1, inplace=True)</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.drop_redundant"><code class="name flex">
<span>def <span class="ident">drop_redundant</span></span>(<span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes features with the same value in all cell.
Drops feature If Nan is the second unique class as well.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>DataFrame or named series</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>DataFrame</code> or <code>named</code> <code>series</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_redundant(data):
    &#39;&#39;&#39;
    Removes features with the same value in all cell. 
    Drops feature If Nan is the second unique class as well.
    Parameters:
        data: DataFrame or named series
    
    Returns:
        DataFrame or named series
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    #get columns
    cols_2_drop = _nan_in_class(data)
    print(&#34;Dropped {}&#34;.format(cols_2_drop))
    data.drop(cols_2_drop, axis=1, inplace=True)</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.fill_missing_cats"><code class="name flex">
<span>def <span class="ident">fill_missing_cats</span></span>(<span>data=None, cat_features=None, missing_encoding=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Fill missing values using the mode of the categorical features.
Parameters:</p>
<hr>
<p>data: DataFrame or name Series.
Data set to perform operation on.
cat_features: List, Series, Array.
categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.
missing_encoding: List, Series, Array.
Values used in place of missing. Popular formats are [-1, -999, -99, '', ' ']</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_missing_cats(data=None, cat_features=None, missing_encoding=None):
    &#39;&#39;&#39;
    Fill missing values using the mode of the categorical features.
    Parameters:
    ----------
    data: DataFrame or name Series.
        Data set to perform operation on.
    cat_features: List, Series, Array.
        categorical features to perform operation on. If not provided, we automatically infer the categoricals from the dataset.
    missing_encoding: List, Series, Array.
            Values used in place of missing. Popular formats are [-1, -999, -99, &#39;&#39;, &#39; &#39;]
    &#39;&#39;&#39;

    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)

    if cat_features is None:
        cat_features = get_cat_feats(data)

    temp_data = data.copy()
    #change all possible missing values to NaN
    if missing_encoding is None:
        missing_encoding = [&#39;&#39;, &#39; &#39;, -99, -999]

    temp_data.replace(missing_encoding, np.NaN, inplace=True)
    
    for col in cat_features:
        most_freq = temp_data[col].mode()[0]
        temp_data[col] = temp_data[col].replace(np.NaN, most_freq)
    
    return temp_data</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.fill_missing_num"><code class="name flex">
<span>def <span class="ident">fill_missing_num</span></span>(<span>data=None, features=None, method='mean')</span>
</code></dt>
<dd>
<section class="desc"><p>fill missing values in numerical columns with specified [method] value
Parameters:</p>
<hr>
<p>data: DataFrame or name Series.
The data set to fill
features: list.
List of columns to fill
method: str, Default 'mean'
method to use in calculating fill value.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_missing_num(data=None, features=None, method=&#39;mean&#39;):
    &#39;&#39;&#39;
    fill missing values in numerical columns with specified [method] value
    Parameters:
    ----------
    data: DataFrame or name Series.
        The data set to fill
    features: list.
        List of columns to fill
    method: str, Default &#39;mean&#39;
        method to use in calculating fill value.
    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if features is None:
        #get numerical features with missing values
        num_feats = get_num_feats(data)
        temp_data = data[num_feats].isna().sum()
        features = list(temp_data[num_feats][temp_data[num_feats] &gt; 0].index)
        print(&#34;Found {} with missing values.&#34;.format(features))

    for feat in features:
        if method is &#39;mean&#39;:
            mean = data[feat].mean()
            data[feat].fillna(mean, inplace=True)
        elif method is &#39;median&#39;:
            median = data[feat].median()
            data[feat].fillna(median, inplace=True)
        elif method is &#39;mode&#39;:
            mode = data[feat].mode()[0]
            data[feat].fillna(mode, inplace=True)
   
    return &#34;Filled all missing values successfully&#34;</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.get_location_center"><code class="name flex">
<span>def <span class="ident">get_location_center</span></span>(<span>point1, point2)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the center between two points.</p>
<p>point1: list, series, scalar
End point latitude of the location.
long1: list, series, scalar
Start point longitude of the location.
long2: list, series, scalar
End point longitude of the location.</p>
<p>Returns: Series
The center between point1 and point2</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_location_center(point1, point2):
    &#39;&#39;&#39;
    Calculates the center between two points.

    point1: list, series, scalar
        End point latitude of the location.
    long1: list, series, scalar
        Start point longitude of the location.
    long2: list, series, scalar
        End point longitude of the location.

    Returns: Series
        The center between point1 and point2
    
    &#39;&#39;&#39;
    center = (point1 + point2) / 2
    center_df = pd.Series(center)
    return center_df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.get_qcut"><code class="name flex">
<span>def <span class="ident">get_qcut</span></span>(<span>data=None, col=None, q=None, duplicates='drop', return_type='float64')</span>
</code></dt>
<dd>
<section class="desc"><p>Cuts a series into bins using the pandas qcut function
and returns the resulting bins as a series for merging.
Parameter:</p>
<hr>
<p>data: DataFrame, named Series
Data set to perform operation on.
col: str
column to cut/binnarize.
q: integer or array of quantiles
Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately
array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.
duplicates: Default 'drop',
If bin edges are not unique drop non-uniques.
return_type: dtype, Default (float64)
Dtype of series to return. One of [float64, str, int64]</p>
<h2 id="returns">Returns:</h2>
<p>Series, 1D-Array
binned series</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_qcut(data=None, col=None, q=None, duplicates=&#39;drop&#39;, return_type=&#39;float64&#39;):
    &#39;&#39;&#39;
    Cuts a series into bins using the pandas qcut function
    and returns the resulting bins as a series for merging.
    Parameter:
    ----------
    data: DataFrame, named Series
        Data set to perform operation on.
    col: str
        column to cut/binnarize.
    q: integer or array of quantiles
        Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately
        array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.
    duplicates: Default &#39;drop&#39;,
        If bin edges are not unique drop non-uniques.
    return_type: dtype, Default (float64)
        Dtype of series to return. One of [float64, str, int64]
    
    Returns:
    --------
    Series, 1D-Array
        binned series
    &#39;&#39;&#39;

    temp_df = pd.qcut(data[col], q=q, duplicates=duplicates).to_frame().astype(&#39;str&#39;)
    #retrieve only the qcut categories
    df = temp_df[col].str.split(&#39;,&#39;).apply(lambda x: x[0][1:]).astype(return_type)
    
    return df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.haversine_distance"><code class="name flex">
<span>def <span class="ident">haversine_distance</span></span>(<span>lat1, long1, lat2, long2)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the Haversine distance between two location with latitude and longitude.
The haversine distance is the great-circle distance between two points on a sphere given their longitudes and latitudes.</p>
<p>Parameter:
lat1: scalar,float
Start point latitude of the location.
lat2: scalar,float
End point latitude of the location.
long1: scalar,float
Start point longitude of the location.
long2: scalar,float
End point longitude of the location.</p>
<p>Returns: Series
The Harversine distance between (lat1, lat2), (long1, long2)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def haversine_distance(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Haversine distance between two location with latitude and longitude.
    The haversine distance is the great-circle distance between two points on a sphere given their longitudes and latitudes.
    
    Parameter:
    lat1: scalar,float
        Start point latitude of the location.
    lat2: scalar,float 
        End point latitude of the location.
    long1: scalar,float
        Start point longitude of the location.
    long2: scalar,float 
        End point longitude of the location.

    Returns: Series
        The Harversine distance between (lat1, lat2), (long1, long2)
    
    &#39;&#39;&#39;

    lat1, long1, lat2, long2 = map(np.radians, (lat1, long1, lat2, long2))
    AVG_EARTH_RADIUS = 6371  # in km
    lat = lat2 - lat1
    lng = long2 - long1
    distance = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2
    harvesine_distance = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(distance))
    harvesine_distance_df = pd.Series(harvesine_distance)
    return harvesine_distance_df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.manhattan_distance"><code class="name flex">
<span>def <span class="ident">manhattan_distance</span></span>(<span>lat1, long1, lat2, long2)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates the Manhattan distance between two points.
It is the sum of horizontal and vertical distance between any two points given their latitudes and longitudes.
Parameter:
lat1: scalar,float
Start point latitude of the location.
lat2: scalar,float
End point latitude of the location.
long1: scalar,float
Start point longitude of the location.
long2: scalar,float
End point longitude of the location.</p>
<p>Returns: Series
The Manhattan distance between (lat1, lat2) and (long1, long2)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def manhattan_distance(lat1, long1, lat2, long2):
    &#39;&#39;&#39;
    Calculates the Manhattan distance between two points.
    It is the sum of horizontal and vertical distance between any two points given their latitudes and longitudes. 
    Parameter:
    lat1: scalar,float
        Start point latitude of the location.
    lat2: scalar,float 
        End point latitude of the location.
    long1: scalar,float
        Start point longitude of the location.
    long2: scalar,float 
        End point longitude of the location.

    Returns: Series
        The Manhattan distance between (lat1, lat2) and (long1, long2)
    
    &#39;&#39;&#39;
    a = np.abs(lat2 -lat1)
    b = np.abs(long1 - long2)
    manhattan_distance = a + b
    manhattan_distance_df = pd.Series(manhattan_distance)
    return manhattan_distance_df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.merge_groupby"><code class="name flex">
<span>def <span class="ident">merge_groupby</span></span>(<span>data=None, cat_features=None, statistics=None, col_to_merge=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a groupby on the specified categorical features and merges
the result to the original dataframe.</p>
<h2 id="parameter">Parameter:</h2>
<p>data: DataFrame
Data set to perform operation on.
cat_features: list, series, 1D-array
categorical features to groupby.
statistics: list, series, 1D-array, Default ['mean', 'count]
aggregates to perform on grouped data.
col_to_merge: str
The column to merge on the dataset. Must be present in the data set.</p>
<h2 id="returns">Returns</h2>
<p>Merged dataframe.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_groupby(data=None, cat_features=None, statistics=None, col_to_merge=None):
    &#39;&#39;&#39;
    Performs a groupby on the specified categorical features and merges
    the result to the original dataframe.

    Parameter:
    ----------
    data: DataFrame
        Data set to perform operation on.
    cat_features: list, series, 1D-array
        categorical features to groupby.
    statistics: list, series, 1D-array, Default [&#39;mean&#39;, &#39;count]
        aggregates to perform on grouped data.
    col_to_merge: str
        The column to merge on the dataset. Must be present in the data set.
    Returns:
        Merged dataframe.

    &#39;&#39;&#39;
    if data is None:
        raise ValueError(&#34;data: Expecting a DataFrame/ numpy2d array, got &#39;None&#39;&#34;)
    
    if statistics is None:     
        statistics = [&#39;mean&#39;, &#39;count&#39;]
    
    if cat_features is None:
        cat_features = get_num_feats(data)

    if col_to_merge is None:
        raise ValueError(&#34;col_to_merge: Expecting a string [column to merge on], got &#39;None&#39;&#34;)

    
    df = data.copy()
    
    for cat in cat_features:      
        temp = df.groupby([cat]).agg(statistics)[col_to_merge]
        #rename columns
        temp = temp.rename(columns={&#39;mean&#39;: cat + &#39;_&#39; + col_to_merge + &#39;_mean&#39;, &#39;count&#39;: cat + &#39;_&#39; + col_to_merge +  &#34;_count&#34;})
        #merge the data sets
        df = df.merge(temp, how=&#39;left&#39;, on=cat)
    
    
    return df</code></pre>
</details>
</dd>
<dt id="datasist.feature_engineering.to_date"><code class="name flex">
<span>def <span class="ident">to_date</span></span>(<span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Automatically convert all date time columns to pandas Datetime format</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_date(data):
    &#39;&#39;&#39;
    Automatically convert all date time columns to pandas Datetime format
    &#39;&#39;&#39;

    date_cols = get_date_cols(data)
    for col in date_cols:
        data[col] = pd.to_datetime(data[col])
    
    return data</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="datasist" href="index.html">datasist</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="datasist.feature_engineering.bearing" href="#datasist.feature_engineering.bearing">bearing</a></code></li>
<li><code><a title="datasist.feature_engineering.create_balanced_data" href="#datasist.feature_engineering.create_balanced_data">create_balanced_data</a></code></li>
<li><code><a title="datasist.feature_engineering.drop_missing" href="#datasist.feature_engineering.drop_missing">drop_missing</a></code></li>
<li><code><a title="datasist.feature_engineering.drop_redundant" href="#datasist.feature_engineering.drop_redundant">drop_redundant</a></code></li>
<li><code><a title="datasist.feature_engineering.fill_missing_cats" href="#datasist.feature_engineering.fill_missing_cats">fill_missing_cats</a></code></li>
<li><code><a title="datasist.feature_engineering.fill_missing_num" href="#datasist.feature_engineering.fill_missing_num">fill_missing_num</a></code></li>
<li><code><a title="datasist.feature_engineering.get_location_center" href="#datasist.feature_engineering.get_location_center">get_location_center</a></code></li>
<li><code><a title="datasist.feature_engineering.get_qcut" href="#datasist.feature_engineering.get_qcut">get_qcut</a></code></li>
<li><code><a title="datasist.feature_engineering.haversine_distance" href="#datasist.feature_engineering.haversine_distance">haversine_distance</a></code></li>
<li><code><a title="datasist.feature_engineering.manhattan_distance" href="#datasist.feature_engineering.manhattan_distance">manhattan_distance</a></code></li>
<li><code><a title="datasist.feature_engineering.merge_groupby" href="#datasist.feature_engineering.merge_groupby">merge_groupby</a></code></li>
<li><code><a title="datasist.feature_engineering.to_date" href="#datasist.feature_engineering.to_date">to_date</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
